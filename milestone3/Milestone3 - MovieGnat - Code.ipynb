{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team NeuralNetflix\n",
    "## Milestone 3 - Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here are the steps to take:**\n",
    "\n",
    "1. We first run Part 1 to grab all movies and keywords from TMDB API and put them into the CSV file titled: *tmdb-movies-1-to-400.csv*, *tmdb-movies-401-to-800.csv*, etc.\n",
    "\n",
    "2. Run Part 2 to grab all the IMDB ids from the TMDB ids provided as the .csv file we generated in Step 1. Write the output into another CSV titled: *imdb-ids-1-to-400.csv*, *imdb-ids-401-to-800.csv*, etc.\n",
    "\n",
    "3. Run Part 3 to grab features from IMDB API given IMDB ids provided as the .csv file generated in Step 2. Write the output into another CSV titles: *imdb-features-1-to-400.csv*, *imdb-features-401-to-800.csv*, etc.\n",
    "\n",
    "4. Run part 4 to merge all the CSV files and output to a single csv file.\n",
    "\n",
    "5. Run part 5 to clean up columns to choose the right ones for Milestone 3\n",
    "\n",
    "6. Based on the genre correlation heatmap from Milestone 1, we combine some genres together to create 7 genre groups\n",
    "\n",
    "7. Run part 7 to perform one hot encoding of genre groups \n",
    "\n",
    "8. Run part 8 to split full dataset to 60% - training set(inbalanced) & 40% - testing set\n",
    "\n",
    "9. Run part 9 to create balanced traning set\n",
    "\n",
    "10. To do - add model 1,2,3 code and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PART 1:  STANDALONE TO GRAB ALL MOVIES AND KEYWORDS\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "#########################################################\n",
    "'''\n",
    "BASE STUFF THAT IS ALSO DEFINED ON TOP\n",
    "'''\n",
    "def requestResults(url):\n",
    "    r = requests.get(BASE_URL + url + \"&api_key=\" + API_KEY)\n",
    "    return r.json()\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://api.themoviedb.org/3/\"\n",
    "API_KEY = \"9767d17413ec9d9729c2cca238df02da\"\n",
    "GENRE_MAP = {}\n",
    "for g in requestResults(\"genre/movie/list?x=1\")[u'genres']:\n",
    "    GENRE_MAP[g['id']] = g['name']\n",
    "    \n",
    "#########################################################\n",
    "\n",
    "\n",
    "def _getKeywordsStringById(movie_id):\n",
    "    \n",
    "    keywords_dict = requestResults(\"movie/\" + str(movie_id) + \"/keywords?language=en-US\")\n",
    "    if u'keywords' not in keywords_dict:\n",
    "        return ''\n",
    "    keywords_dict = keywords_dict[u'keywords']\n",
    "    kstring = ''\n",
    "    for k in keywords_dict:\n",
    "        kstring += k[u'name'] + ','\n",
    "    return str(kstring.encode('utf-8').strip())[:-1]\n",
    "\n",
    "def _tidyRow(m, keywords):\n",
    "    # Makes sure the row of movie is well-formatted\n",
    "    output = {}\n",
    "    for k in m:\n",
    "        typem = type(m[k])\n",
    "        k = str(k)\n",
    "        if typem == str or typem == unicode:\n",
    "            output[k] = m[k].encode('utf-8').strip()\n",
    "        else:\n",
    "            output[k] = m[k]\n",
    "    output['keywords'] = keywords\n",
    "    return output\n",
    "\n",
    "def downloadMoviesToCSV(start_page, increment, filename):\n",
    "    genre_count = {}\n",
    "    \n",
    "    with open(filename, 'w') as csvfile:\n",
    "        fieldnames = ['id', 'genre_ids', 'poster_path', 'title', 'overview', 'release_date', \n",
    "                      'popularity', 'original_title', 'backdrop_path', 'keywords', \n",
    "                     'vote_count', 'video', 'adult', 'vote_average', 'original_language']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Get keywords for movies\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # TMDB limits 4 requests per second\n",
    "        hit = 3 # Once hit reaches 0, call timer and reset hit to 3\n",
    "        \n",
    "        for p in range(start_page,start_page+increment): \n",
    "            results_p = requestResults(\"discover/movie?sort_by=popularity.desc&page=\" + str(p))[u'results']\n",
    "            hit -= 1\n",
    "            if hit <= 0:\n",
    "                hit = 3\n",
    "                time.sleep(1)\n",
    "\n",
    "            # Write to CSV\n",
    "            for m in results_p:\n",
    "                mid = m[u'id']\n",
    "                keywords = _getKeywordsStringById(mid)\n",
    "                hit -= 1\n",
    "                if hit <= 0:\n",
    "                    hit = 3\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                row = _tidyRow(m, keywords)\n",
    "                writer.writerow(row)\n",
    "            print('%d pages done' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run Part 1: REMEMBER TO CHANGE start_page to your start page, don't have to change increment\n",
    "downloadMoviesToCSV(start_page=1, increment=400, filename='tmdb-movies-1-to-400.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PART 2: STANDALONE THAT TAKES IN .CSV FILE AND GETS ALL IMDB IDS in a separate file\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "#########################################################\n",
    "'''\n",
    "BASE STUFF THAT IS ALSO DEFINED ON TOP\n",
    "'''\n",
    "def requestResults(url):\n",
    "    r = requests.get(BASE_URL + url + \"&api_key=\" + API_KEY)\n",
    "    return r.json()\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://api.themoviedb.org/3/\"\n",
    "API_KEY = \"9767d17413ec9d9729c2cca238df02da\"\n",
    "GENRE_MAP = {}\n",
    "for g in requestResults(\"genre/movie/list?x=1\")[u'genres']:\n",
    "    GENRE_MAP[g['id']] = g['name']\n",
    "    \n",
    "#########################################################\n",
    "\n",
    "def downloadIMDBIds(input_filename, output_filename):\n",
    "    df = pd.read_csv(input_filename)\n",
    "\n",
    "    with open(output_filename, 'w') as csvfile:\n",
    "        fieldnames = ['id', 'imdb_id']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # TMDB limits 4 requests per second\n",
    "        hit = 3 # Once hit reaches 0, call timer and reset hit to 3\n",
    "\n",
    "        count = 0\n",
    "        for tmid in df['id']:\n",
    "            count += 1\n",
    "            results = requestResults('movie/' + str(tmid) + '?x=1')\n",
    "            if u'imdb_id' not in results or results[u'imdb_id'] is None:\n",
    "                continue\n",
    "            imid = results[u'imdb_id'].strip('tt')\n",
    "            row = {'id': tmid, 'imdb_id': imid}\n",
    "            writer.writerow(row)\n",
    "            hit -= 1\n",
    "            if hit <= 0:\n",
    "                hit = 3\n",
    "                time.sleep(1)\n",
    "            if count % 200 == 0:\n",
    "                print 'done with %d movies' % count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run Part 2: Get imdb ids from tmdb ids input csv file\n",
    "downloadIMDBIds(input_filename='tmdb-movies-1-to-400.csv', output_filename='imdb-ids-1-to-400.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PART 3: STANDALONE THAT TAKES IN IMDB IDs and gets IMDB features\n",
    "'''\n",
    "Make sure you have IMDB installed.\n",
    "- Go to: http://imdbpy.sourceforge.net/\n",
    "- Download and unzip, then cd into it and make sure there is a setup.py file\n",
    "- Run python setup.py install\n",
    "- You're done! It's globally installed.\n",
    "'''\n",
    "import imdb\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def getIMDBFeatures(input_filename, output_filename, start, increment):\n",
    "\n",
    "    # Note: This cannot be terminated via the stop button (interrupt the kernel), \n",
    "    # got to restart the kernel (use rewind button) :(\n",
    "    \n",
    "    ia = imdb.IMDb()\n",
    "    df = pd.read_csv(input_filename)\n",
    "    # Download increment movies at a time\n",
    "    df = df[start:start+increment]\n",
    "    \n",
    "    imids = np.array(df['imdb_id'])\n",
    "\n",
    "    with open(output_filename + '-' + str(start), 'w') as csvfile:\n",
    "        # Grab these features from IMDB\n",
    "        fieldnames = ['imdb_id', 'director', 'imdb_votes', 'certificate', 'num_stunts', 'num_fx']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        count = 0\n",
    "        for imid in imids:\n",
    "            count += 1\n",
    "            # Tries twice because sometimes it fails\n",
    "            for i in range(2):\n",
    "                try:\n",
    "                    movie = ia.get_movie(str(int(imid)))\n",
    "                    director = movie['director'][0]\n",
    "                    imdb_votes = movie['votes']\n",
    "                    certificate = movie['certificates'][-2].split(':')[1]\n",
    "                    num_stunts = len(movie['stunt performer'])\n",
    "                    num_fx = len(movie['special effects department'])\n",
    "                    row = {'imdb_id': imid, 'director': director, 'imdb_votes': imdb_votes, 'certificate': certificate, \n",
    "                          'num_stunts': num_stunts, 'num_fx': num_fx}\n",
    "                    writer.writerow(row)\n",
    "                    break\n",
    "                except:    \n",
    "                    pass\n",
    "            if count % 100 == 0:\n",
    "                print 'Done with %d movies' % count\n",
    "    print 'Done with page %d' % ((start%increment) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run Part 3: Get imdb features from imdb ids\n",
    "\n",
    "# NOTE: This downloads 500 movies at a time and stores each in a different file.\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('imdb-ids-1001-to-1200.csv')\n",
    "N = df.shape[0]\n",
    "increment = 500 # Work on 500 movies at a time\n",
    "end_page = N/increment\n",
    "\n",
    "##############################################################\n",
    "# NOTE: If you are done with page 2 (1000 movies), then change this to 2 the next time you start\n",
    "start_page = 0\n",
    "##############################################################\n",
    "\n",
    "starts = [] \n",
    "for i in range(start_page,end_page): # default starts: [500,1000,1500,2000,2500,..,7500]\n",
    "    starts.append((i+1)*increment)\n",
    "\n",
    "for start in starts: \n",
    "    getIMDBFeatures(input_filename='imdb-ids-1-to-400.csv', output_filename='imdb-features-1-to-400.csv', \n",
    "                    start=start, increment=increment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PART 4: Merge all and output CSV file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# NOTE: Change to your filepath and start and end movie\n",
    "prefix_filepath = ''\n",
    "start = 1\n",
    "end = 1000\n",
    "\n",
    "# Merge all imdb features into one\n",
    "imdb_features = pd.read_csv(prefix_filepath + 'imdb-features-'+str(start)+'-to-'+str(end)+'.csv-500')\n",
    "for p in range(2,8):\n",
    "    imdb_features_ = pd.read_csv(prefix_filepath + 'imdb-features-'+str(start)+'-to-'+str(end)+'.csv-' + str(p*500))\n",
    "    imdb_features = imdb_features.append(imdb_features_)\n",
    "\n",
    "    \n",
    "# Merge imdb ids with imdb features\n",
    "imdb_ids = pd.read_csv(prefix_filepath + 'imdb-ids-'+str(start)+'-to-'+str(end)+'.csv')\n",
    "imdb_ids = imdb_ids.rename(index=str, columns={\"id\": \"tmdb_id\"})\n",
    "imdb_merged = imdb_ids.merge(imdb_features, how='outer', left_on='imdb_id', right_on='imdb_id')\n",
    "imdb_merged = imdb_merged.dropna()\n",
    "\n",
    "# Merge tmdb with imdb_merge\n",
    "tmdb_movies = pd.read_csv(prefix_filepath + 'tmdb-movies-'+str(start)+'-to-'+str(end)+'.csv')\n",
    "tmdb_movies = tmdb_movies.rename(index=str, columns={\"id\": \"tmdb_id\"})\n",
    "full_movies = tmdb_movies.merge(imdb_merged, how='outer', left_on='tmdb_id', right_on='tmdb_id')\n",
    "full_movies = full_movies.dropna()\n",
    "\n",
    "# Output this to CSV of full movies\n",
    "full_movies.to_csv('full-movies-'+str(start)+'-to-'+str(end)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PART 5: Clean up columns to choose the right ones for Milestone 3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "#########################################################\n",
    "'''\n",
    "BASE STUFF THAT IS ALSO DEFINED ON TOP\n",
    "'''\n",
    "def requestResults(url):\n",
    "    r = requests.get(BASE_URL + url + \"&api_key=\" + API_KEY)\n",
    "    return r.json()\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://api.themoviedb.org/3/\"\n",
    "API_KEY = \"9767d17413ec9d9729c2cca238df02da\"\n",
    "GENRE_MAP = {}\n",
    "for g in requestResults(\"genre/movie/list?x=1\")[u'genres']:\n",
    "    GENRE_MAP[g['id']] = g['name']\n",
    "    \n",
    "#########################################################\n",
    "\n",
    "# Merge the few tmdb-movies files together\n",
    "df1 = pd.read_csv('full-movies-1-to-400.csv')\n",
    "df2 = pd.read_csv('full-movies-401-to-800.csv')\n",
    "df3 = pd.read_csv('full-movies-801-to-1000.csv')\n",
    "df = (df1.append(df2)).append(df3)\n",
    "\n",
    "# Choose only columns we need\n",
    "cols = ['genre_ids', 'title', 'poster_path', 'tmdb_id', 'release_date', 'popularity', 'keywords', 'vote_count',\n",
    "       'vote_average', 'director', 'imdb_votes', 'certificate', 'num_stunts', 'num_fx']\n",
    "df = df[cols]\n",
    "\n",
    "# Break down release date into month and year\n",
    "datesplit = df['release_date'].str.split('-')\n",
    "years = [int(d[0]) for d in datesplit]\n",
    "months = [int(d[1]) for d in datesplit]\n",
    "df['year'] = years\n",
    "df['month'] = months\n",
    "del df['release_date']\n",
    "\n",
    "# Split year into decades\n",
    "base = 1910\n",
    "mod_var = 10\n",
    "years = np.array(df['year'])\n",
    "decades = (years - base) / mod_var\n",
    "df['decade'] = decades\n",
    "df.head()\n",
    "del df['year']\n",
    "\n",
    "# Tidy up the certificate and one-hot encoding\n",
    "\n",
    "# Look at the certs\n",
    "cert_ratings = np.unique(df['certificate'], return_counts=True)[0]\n",
    "cert_counts = np.unique(df['certificate'], return_counts=True)[1]\n",
    "df_cert = pd.DataFrame(columns=['name', 'count'])\n",
    "df_cert['name'] = cert_ratings\n",
    "df_cert['count'] = cert_counts\n",
    "df_cert.sort_values('count', ascending=False)\n",
    "\n",
    "# Relabel certs\n",
    "df = df[~df['certificate'].isin(['(Banned)','10'])]\n",
    "certs_formatted = []\n",
    "certs = df['certificate']\n",
    "for c in certs:\n",
    "    if c in ['Tous publics', 'U', 'T', 'M/6', 'L', 'G', 'Btl', 'All', 'AL', 'AA', 'A', '0']: # U\n",
    "        certs_formatted.append('U') \n",
    "    elif c in ['Unrated', 'Not Rated', 'H', 'B', ]: # Unrated\n",
    "        certs_formatted.append('Unrated')\n",
    "    elif c in ['X', 'TV-MA', 'R21', 'R(A)', 'IIB', '(Banned)', 'R']: # R\n",
    "        certs_formatted.append('R')\n",
    "    elif c in ['VM14', 'TV-14', 'R16', 'R-16', 'R-15', 'NC16', \n",
    "               'NC-17', 'NC-16', 'MA15+', 'M/16', 'M/14', 'M', 'K-15', 'B-15', '16', '15A', '15',\n",
    "              '14', '14A']: # 15\n",
    "        certs_formatted.append('15')\n",
    "    elif c in ['TV-Y7', 'TV-G', 'TV-PG', 'Passed', 'PG', 'M/PG', 'K-7', 'IIA', 'GP', 'Approved', \n",
    "              '7', '8', '9', '6', '10']: # PG\n",
    "        certs_formatted.append('PG')\n",
    "    elif c in ['R18+', 'R18', 'R-18', 'M18', 'M/18', '18']: # 18\n",
    "        certs_formatted.append('18')\n",
    "    elif c in ['R13', 'R-13', 'R-12', 'PG12', 'PG13', 'PG-12', 'PG-13', \n",
    "               'P13', 'M/12', 'K-13', 'K-12', 'K-11', '13+', '13', '12A', '12+', '12', '11']: # 12\n",
    "        certs_formatted.append('12')\n",
    "    else: # If we miss out, which is unlikely, just mark Unrated\n",
    "        print c\n",
    "certs_formatted = np.array(certs_formatted)\n",
    "df['certificate'] = certs_formatted\n",
    "\n",
    "df.to_csv('full-movies-merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PART 6: Based on the genre correlation heatmap from Milestone 1 \n",
    "#to create 7 genre groups with correponding genre ids\n",
    "\n",
    "combine_genre = {'group1': [\"War\",\"History\"], \n",
    "                 'group2': [\"Crime\",\"Mystery\",\"Thriller\",\"Drama\",\"Horror\"], \n",
    "                 'group3': [\"Fantasy\"], \n",
    "                 'group4': [\"Family\",\"Animation\"], \n",
    "                 'group5': [\"Romance\",\"Music\"], \n",
    "                 'group6': [\"Science Fiction\",\"Action\",\"Adventure\"], \n",
    "                 'group7': [\"Comedy\"]}\n",
    "# no western, tv movie, and documentary due to the insignificant number of movies for each genre\n",
    "\n",
    "combine_genre_ids = {'group1': ['10752', '36'], \n",
    "                 'group2': ['80', '9648', '53', '18','27'], \n",
    "                 'group3': ['14'], \n",
    "                 'group4': ['10751','16'], \n",
    "                 'group5': ['10749', '10402'], \n",
    "                 'group6': ['878', '28', '12'], \n",
    "                 'group7': ['35']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PART 7: One hot encoding of genre groups \n",
    "\n",
    "full_df = pd.read_csv(\"full-movies-merged.csv\")\n",
    "\n",
    "def one_hot_encoding_genre_group(df):\n",
    "    num_row = len(df)\n",
    "    group_df = pd.DataFrame()\n",
    "    for i in range(7):\n",
    "        col_name = combine_genre_ids.keys()[i]\n",
    "        print(col_name)\n",
    "        col_genre = combine_genre_ids.values()[i]\n",
    "        #print(col_genre)\n",
    "        group_df[col_name] = [0]*num_row\n",
    "        for row in range(num_row):\n",
    "            genres_id = df['genre_ids'][row][1:-1].split(\",\")\n",
    "            genres_list = []\n",
    "            for i in genres_id:\n",
    "                genres_list.append(str(i.strip()))\n",
    "            #print(genres_list)\n",
    "            overlap = bool(set(genres_list) & set(col_genre))\n",
    "            if overlap == True:\n",
    "                #print(\"there is a match\")\n",
    "                group_df[col_name][row] = 1\n",
    "        print(col_name + \" done\")\n",
    "    return(group_df)\n",
    "\n",
    "group_df = one_hot_encoding_genre_group(full_df)\n",
    "\n",
    "# merge_with_group dataframe includes all the attributes in the original dataset,\n",
    "# along with 7 additional binary-value columns for genre groups\n",
    "\n",
    "merge_with_group = pd.concat([full_df, group_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Part 8: Split full dataset to 60% - training set(inbalanced) & 40% - testing set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "df = merge_with_group\n",
    "counts = []\n",
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "for i in range(1,8):\n",
    "    sub_df = df[df['group' + str(i)] == 1]\n",
    "    count = sub_df.shape[0]\n",
    "    rows = random.sample(sub_df.index, int(count*0.6))\n",
    "    sub_train = sub_df.ix[rows]\n",
    "    sub_test = sub_df.drop(rows)\n",
    "    \n",
    "    train_df = pd.concat([train_df, sub_train], axis=0)\n",
    "    test_df = pd.concat([test_df, sub_test], axis=0)\n",
    "    \n",
    "    counts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Part 9: Run Oversampling/Undersampling algorithm to create balanced traning set\n",
    "\n",
    "# MovieGnat Oversampling/Undersampling algorithm\n",
    "df = train_df_no_dup\n",
    "\n",
    "counts = []\n",
    "# Get counts of each label\n",
    "for i in range(1,8):\n",
    "    count = df[df['group' + str(i)] == 1].shape[0]\n",
    "    counts.append(count)\n",
    "\n",
    "# Target number of samples to hit from oversampling and undersampling\n",
    "target_count = int(np.mean(counts))\n",
    "\n",
    "print 'Counts: \\n', counts\n",
    "print '\\nTarget count: ', target_count\n",
    "\n",
    "sampled_df = pd.DataFrame(columns=df.columns.values)\n",
    "\n",
    "# Oversample the minority, and heavily undersample the majority\n",
    "for l in range(1,8):\n",
    "    # If above the mean, target_count is /2\n",
    "    df_ = df[df['group' + str(l)] == 1]\n",
    "    target_count_ = target_count/2 if df_.shape[0] > target_count else target_count\n",
    "    df_sample = df_.sample(n=target_count_, replace=True)\n",
    "    sampled_df = sampled_df.append(df_sample)\n",
    "    \n",
    "# Print out counts of each now\n",
    "sampled_counts = []\n",
    "for l in range(1,8):\n",
    "    count = sampled_df[sampled_df['group' + str(l)] == 1].shape[0]\n",
    "    sampled_counts.append(count)\n",
    "sampled_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
