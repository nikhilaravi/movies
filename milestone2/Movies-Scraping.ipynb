{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movies Scraping from TMDB and IMDB\n",
    "## Team MovieGnat\n",
    "\n",
    "**Here are the steps to take:**\n",
    "\n",
    "1. Run Part 1 to grab all movies and keywords from TMDB API and put them into your CSV file titled: *tmdb-movies-1-to-400.csv*, *tmdb-movies-401-to-800.csv*, etc.\n",
    "\n",
    "2. Run Part 2 to grab all the IMDB ids from the TMDB ids provided as the .csv file you generated in Step 1. Write the output into another CSV titled: *imdb-ids-1-to-400.csv*, *imdb-ids-401-to-800.csv*, etc.\n",
    "\n",
    "3. Run Part 3 to grab features from IMDB API given IMDB ids provided as the .csv file generated in Step 2. Write the output into another CSV titles: *imdb-features-1-to-400.csv*, *imdb-features-401-to-800.csv*, etc.\n",
    "\n",
    "4. (Not yet done) We will have to map the IMDB features back to the part 1 dataset of TMDB features based on ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PART 1:  STANDALONE TO GRAB ALL MOVIES AND KEYWORDS\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "#########################################################\n",
    "'''\n",
    "BASE STUFF THAT IS ALSO DEFINED ON TOP\n",
    "'''\n",
    "def requestResults(url):\n",
    "    r = requests.get(BASE_URL + url + \"&api_key=\" + API_KEY)\n",
    "    return r.json()\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://api.themoviedb.org/3/\"\n",
    "API_KEY = \"9767d17413ec9d9729c2cca238df02da\"\n",
    "GENRE_MAP = {}\n",
    "for g in requestResults(\"genre/movie/list?x=1\")[u'genres']:\n",
    "    GENRE_MAP[g['id']] = g['name']\n",
    "    \n",
    "#########################################################\n",
    "\n",
    "\n",
    "def _getKeywordsStringById(movie_id):\n",
    "    \n",
    "    keywords_dict = requestResults(\"movie/\" + str(movie_id) + \"/keywords?language=en-US\")\n",
    "    if u'keywords' not in keywords_dict:\n",
    "        return ''\n",
    "    keywords_dict = keywords_dict[u'keywords']\n",
    "    kstring = ''\n",
    "    for k in keywords_dict:\n",
    "        kstring += k[u'name'] + ','\n",
    "    return str(kstring.encode('utf-8').strip())[:-1]\n",
    "\n",
    "def _tidyRow(m, keywords):\n",
    "    # Makes sure the row of movie is well-formatted\n",
    "    output = {}\n",
    "    for k in m:\n",
    "        typem = type(m[k])\n",
    "        k = str(k)\n",
    "        if typem == str or typem == unicode:\n",
    "            output[k] = m[k].encode('utf-8').strip()\n",
    "        else:\n",
    "            output[k] = m[k]\n",
    "    output['keywords'] = keywords\n",
    "    return output\n",
    "\n",
    "def downloadMoviesToCSV(start_page, increment, filename):\n",
    "    genre_count = {}\n",
    "    \n",
    "    with open(filename, 'w') as csvfile:\n",
    "        fieldnames = ['id', 'genre_ids', 'poster_path', 'title', 'overview', 'release_date', \n",
    "                      'popularity', 'original_title', 'backdrop_path', 'keywords', \n",
    "                     'vote_count', 'video', 'adult', 'vote_average', 'original_language']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Get keywords for movies\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # TMDB limits 4 requests per second\n",
    "        hit = 3 # Once hit reaches 0, call timer and reset hit to 3\n",
    "        \n",
    "        for p in range(start_page,start_page+increment): \n",
    "            results_p = requestResults(\"discover/movie?sort_by=popularity.desc&page=\" + str(p))[u'results']\n",
    "            hit -= 1\n",
    "            if hit <= 0:\n",
    "                hit = 3\n",
    "                time.sleep(1)\n",
    "\n",
    "            # Write to CSV\n",
    "            for m in results_p:\n",
    "                mid = m[u'id']\n",
    "                keywords = _getKeywordsStringById(mid)\n",
    "                hit -= 1\n",
    "                if hit <= 0:\n",
    "                    hit = 3\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                row = _tidyRow(m, keywords)\n",
    "                writer.writerow(row)\n",
    "            print('%d pages done' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801 pages done\n",
      "802 pages done\n",
      "803 pages done\n",
      "804 pages done\n",
      "805 pages done\n",
      "806 pages done\n",
      "807 pages done\n",
      "808 pages done\n",
      "809 pages done\n",
      "810 pages done\n",
      "811 pages done\n",
      "812 pages done\n",
      "813 pages done\n",
      "814 pages done\n",
      "815 pages done\n",
      "816 pages done\n",
      "817 pages done\n",
      "818 pages done\n",
      "819 pages done\n",
      "820 pages done\n",
      "821 pages done\n",
      "822 pages done\n",
      "823 pages done\n",
      "824 pages done\n",
      "825 pages done\n",
      "826 pages done\n",
      "827 pages done\n",
      "828 pages done\n",
      "829 pages done\n",
      "830 pages done\n",
      "831 pages done\n",
      "832 pages done\n",
      "833 pages done\n",
      "834 pages done\n",
      "835 pages done\n",
      "836 pages done\n",
      "837 pages done\n",
      "838 pages done\n",
      "839 pages done\n",
      "840 pages done\n",
      "841 pages done\n",
      "842 pages done\n",
      "843 pages done\n",
      "844 pages done\n",
      "845 pages done\n",
      "846 pages done\n",
      "847 pages done\n",
      "848 pages done\n",
      "849 pages done\n",
      "850 pages done\n",
      "851 pages done\n",
      "852 pages done\n",
      "853 pages done\n",
      "854 pages done\n",
      "855 pages done\n",
      "856 pages done\n",
      "857 pages done\n",
      "858 pages done\n",
      "859 pages done\n",
      "860 pages done\n",
      "861 pages done\n",
      "862 pages done\n",
      "863 pages done\n",
      "864 pages done\n",
      "865 pages done\n",
      "866 pages done\n",
      "867 pages done\n",
      "868 pages done\n",
      "869 pages done\n",
      "870 pages done\n",
      "871 pages done\n",
      "872 pages done\n",
      "873 pages done\n",
      "874 pages done\n",
      "875 pages done\n",
      "876 pages done\n",
      "877 pages done\n",
      "878 pages done\n",
      "879 pages done\n",
      "880 pages done\n",
      "881 pages done\n",
      "882 pages done\n",
      "883 pages done\n",
      "884 pages done\n",
      "885 pages done\n",
      "886 pages done\n",
      "887 pages done\n",
      "888 pages done\n",
      "889 pages done\n",
      "890 pages done\n",
      "891 pages done\n",
      "892 pages done\n",
      "893 pages done\n",
      "894 pages done\n",
      "895 pages done\n",
      "896 pages done\n",
      "897 pages done\n",
      "898 pages done\n",
      "899 pages done\n",
      "900 pages done\n",
      "901 pages done\n",
      "902 pages done\n",
      "903 pages done\n",
      "904 pages done\n",
      "905 pages done\n",
      "906 pages done\n",
      "907 pages done\n",
      "908 pages done\n",
      "909 pages done\n",
      "910 pages done\n",
      "911 pages done\n",
      "912 pages done\n",
      "913 pages done\n",
      "914 pages done\n",
      "915 pages done\n",
      "916 pages done\n",
      "917 pages done\n",
      "918 pages done\n",
      "919 pages done\n",
      "920 pages done\n",
      "921 pages done\n",
      "922 pages done\n",
      "923 pages done\n",
      "924 pages done\n",
      "925 pages done\n",
      "926 pages done\n",
      "927 pages done\n",
      "928 pages done\n",
      "929 pages done\n",
      "930 pages done\n",
      "931 pages done\n",
      "932 pages done\n",
      "933 pages done\n",
      "934 pages done\n",
      "935 pages done\n",
      "936 pages done\n",
      "937 pages done\n",
      "938 pages done\n",
      "939 pages done\n",
      "940 pages done\n",
      "941 pages done\n",
      "942 pages done\n",
      "943 pages done\n",
      "944 pages done\n",
      "945 pages done\n",
      "946 pages done\n",
      "947 pages done\n",
      "948 pages done\n",
      "949 pages done\n",
      "950 pages done\n",
      "951 pages done\n",
      "952 pages done\n",
      "953 pages done\n",
      "954 pages done\n",
      "955 pages done\n",
      "956 pages done\n",
      "957 pages done\n",
      "958 pages done\n",
      "959 pages done\n",
      "960 pages done\n",
      "961 pages done\n",
      "962 pages done\n",
      "963 pages done\n",
      "964 pages done\n",
      "965 pages done\n",
      "966 pages done\n",
      "967 pages done\n",
      "968 pages done\n",
      "969 pages done\n",
      "970 pages done\n",
      "971 pages done\n",
      "972 pages done\n",
      "973 pages done\n",
      "974 pages done\n",
      "975 pages done\n",
      "976 pages done\n",
      "977 pages done\n",
      "978 pages done\n",
      "979 pages done\n",
      "980 pages done\n",
      "981 pages done\n",
      "982 pages done\n",
      "983 pages done\n",
      "984 pages done\n",
      "985 pages done\n",
      "986 pages done\n",
      "987 pages done\n",
      "988 pages done\n",
      "989 pages done\n",
      "990 pages done\n",
      "991 pages done\n",
      "992 pages done\n",
      "993 pages done\n",
      "994 pages done\n",
      "995 pages done\n",
      "996 pages done\n",
      "997 pages done\n",
      "998 pages done\n",
      "999 pages done\n",
      "1000 pages done\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "u'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a764fb9db5c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Run Part 1: REMEMBER TO CHANGE start_page to your start page, don't have to change increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdownloadMoviesToCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_page\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m801\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincrement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tmdb-movies-801-to-1200.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-329236626605>\u001b[0m in \u001b[0;36mdownloadMoviesToCSV\u001b[0;34m(start_page, increment, filename)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_page\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_page\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mincrement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mresults_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequestResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"discover/movie?sort_by=popularity.desc&page=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mhit\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhit\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: u'results'"
     ]
    }
   ],
   "source": [
    "### Run Part 1: REMEMBER TO CHANGE start_page to your start page, don't have to change increment\n",
    "downloadMoviesToCSV(start_page=801, increment=400, filename='tmdb-movies-801-to-1200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PART 2: STANDALONE THAT TAKES IN .CSV FILE AND GETS ALL IMDB IDS in a separate file\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "#########################################################\n",
    "'''\n",
    "BASE STUFF THAT IS ALSO DEFINED ON TOP\n",
    "'''\n",
    "def requestResults(url):\n",
    "    r = requests.get(BASE_URL + url + \"&api_key=\" + API_KEY)\n",
    "    return r.json()\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://api.themoviedb.org/3/\"\n",
    "API_KEY = \"9767d17413ec9d9729c2cca238df02da\"\n",
    "GENRE_MAP = {}\n",
    "for g in requestResults(\"genre/movie/list?x=1\")[u'genres']:\n",
    "    GENRE_MAP[g['id']] = g['name']\n",
    "    \n",
    "#########################################################\n",
    "\n",
    "def downloadIMDBIds(input_filename, output_filename):\n",
    "    df = pd.read_csv(input_filename)\n",
    "\n",
    "    with open(output_filename, 'w') as csvfile:\n",
    "        fieldnames = ['id', 'imdb_id']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # TMDB limits 4 requests per second\n",
    "        hit = 3 # Once hit reaches 0, call timer and reset hit to 3\n",
    "\n",
    "        count = 0\n",
    "        for tmid in df['id']:\n",
    "            count += 1\n",
    "            results = requestResults('movie/' + str(tmid) + '?x=1')\n",
    "            if u'imdb_id' not in results or results[u'imdb_id'] is None:\n",
    "                continue\n",
    "            imid = results[u'imdb_id'].strip('tt')\n",
    "            row = {'id': tmid, 'imdb_id': imid}\n",
    "            writer.writerow(row)\n",
    "            hit -= 1\n",
    "            if hit <= 0:\n",
    "                hit = 3\n",
    "                time.sleep(1)\n",
    "            if count % 200 == 0:\n",
    "                print 'done with %d movies' % count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run Part 2: Get imdb ids from tmdb ids input csv file\n",
    "downloadIMDBIds(input_filename='tmdb-movies-801-to-1200.csv', output_filename='imdb-ids-801-to-1200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PART 3: STANDALONE THAT TAKES IN IMDB IDs and gets IMDB features\n",
    "'''\n",
    "Make sure you have IMDB installed.\n",
    "- Go to: http://imdbpy.sourceforge.net/\n",
    "- Download and unzip, then cd into it and make sure there is a setup.py file\n",
    "- Run python setup.py install\n",
    "- You're done! It's globally installed.\n",
    "'''\n",
    "import imdb\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def getIMDBFeatures(input_filename, output_filename, start, increment):\n",
    "\n",
    "    # Note: This cannot be terminated via the stop button (interrupt the kernel), \n",
    "    # got to restart the kernel (use rewind button) :(\n",
    "    \n",
    "    ia = imdb.IMDb()\n",
    "    df = pd.read_csv(input_filename)\n",
    "    # Download increment movies at a time\n",
    "    df = df[start:start+increment]\n",
    "    \n",
    "    imids = np.array(df['imdb_id'])\n",
    "\n",
    "    with open(output_filename + '-' + str(start), 'w') as csvfile:\n",
    "        # Grab these features from IMDB\n",
    "        fieldnames = ['imdb_id', 'director', 'imdb_votes', 'certificate', 'num_stunts', 'num_fx']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        count = 0\n",
    "        for imid in imids:\n",
    "            count += 1\n",
    "            # Tries twice because sometimes it fails\n",
    "            for i in range(2):\n",
    "                try:\n",
    "                    movie = ia.get_movie(str(int(imid)))\n",
    "                    director = movie['director'][0]\n",
    "                    imdb_votes = movie['votes']\n",
    "                    certificate = movie['certificates'][-2].split(':')[1]\n",
    "                    num_stunts = len(movie['stunt performer'])\n",
    "                    num_fx = len(movie['special effects department'])\n",
    "                    row = {'imdb_id': imid, 'director': director, 'imdb_votes': imdb_votes, 'certificate': certificate, \n",
    "                          'num_stunts': num_stunts, 'num_fx': num_fx}\n",
    "                    writer.writerow(row)\n",
    "                    break\n",
    "                except:    \n",
    "                    pass\n",
    "            if count % 100 == 0:\n",
    "                print 'Done with %d movies' % count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run Part 3: Get imdb features from imdb ids\n",
    "\n",
    "# NOTE: This downloads 500 movies at a time and stores it in a different file.\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('imdb-ids-1-to-400.csv')\n",
    "N = df.shape[0]\n",
    "increment = 500 # Work on 500 movies at a time\n",
    "sections = N/increment\n",
    "\n",
    "# If you stop halfway change the starts array to: [2000,2500,..,7500] (this is when you are done with 1500 movies)\n",
    "starts = [] \n",
    "for i in range(sections): # default starts: [500,1000,1500,2000,2500,..,7500]\n",
    "    starts.append((i+1)*increment)\n",
    "\n",
    "# Output 500 movies in a file at a time\n",
    "for start in starts: \n",
    "    getIMDBFeatures(input_filename='imdb-ids-1-to-400.csv', output_filename='imdb-features-1-to-400.csv', \n",
    "                    start=start, increment=increment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
